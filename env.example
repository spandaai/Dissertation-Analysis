# VLLM_URL = "http://localhost:8000/v1/chat/completions"   # make this an env variable
# VLLM_MODEL_FOR_ANALYSIS = "neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16"              # make this an env variable
# VLLM_MODEL_FOR_EXTRACTION = "neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16"              # make this an env variable
# VLLM_MODEL_FOR_SUMMARY = "neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16"              # make this an env variable
# VLLM_MODEL_FOR_IMAGE = "neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16"            # make this an env variable
# VLLM_MODEL_FOR_SCORING = "neuralmagic/Meta-Llama-3.1-70B-Instruct-quantized.w4a16"              # make this an env variable


OLLAMA_URL = "http://localhost:11434"   # make this an env variable
OLLAMA_MODEL_FOR_ANALYSIS = "llama3.2"               # make this an env variable
OLLAMA_MODEL_FOR_EXTRACTION = "llama3.2"               # make this an env variable
OLLAMA_MODEL_FOR_SUMMARY = "llama3.2"               # make this an env variable
OLLAMA_MODEL_FOR_IMAGE = "llama3.2-vision:111b"
OLLAMA_MODEL_FOR_SCORING = "llama3.1"               # make this an env variable
